{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzL/BaLlh9zGuukvaClGrr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariahwy/Multimodal-Deperession-Detection/blob/main/tensorfusion_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI4MlQJ8tSym",
        "outputId": "97b110f4-3f50-4208-c4aa-e0e9377715c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.init import xavier_uniform, xavier_normal, orthogonal"
      ],
      "metadata": {
        "id": "7hqUgv_it34S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "6XZi69J72G7P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPUOyhRPs_Re"
      },
      "outputs": [],
      "source": [
        "class SubNet(nn.Module):\n",
        "    '''\n",
        "    The subnetwork that is used in TFN for video and audio in the pre-fusion stage\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_size, hidden_size, dropout):\n",
        "        '''\n",
        "        Args:\n",
        "            in_size: input dimension\n",
        "            hidden_size: hidden layer dimension\n",
        "            dropout: dropout probability\n",
        "        Output:\n",
        "            (return value in forward) a tensor of shape (batch_size, hidden_size)\n",
        "        '''\n",
        "        super(SubNet, self).__init__()\n",
        "        self.norm = nn.BatchNorm1d(in_size)\n",
        "        self.drop = nn.Dropout(p=dropout)\n",
        "        self.linear_1 = nn.Linear(in_size, hidden_size)\n",
        "        self.linear_2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear_3 = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            x: tensor of shape (batch_size, in_size)\n",
        "        '''\n",
        "        normed = self.norm(x)\n",
        "        dropped = self.drop(normed)\n",
        "        y_1 = F.relu(self.linear_1(dropped))\n",
        "        y_2 = F.relu(self.linear_2(y_1))\n",
        "        y_3 = F.relu(self.linear_3(y_2))\n",
        "\n",
        "        return y_3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextSubNet(nn.Module):\n",
        "    '''\n",
        "    The LSTM-based subnetwork that is used in TFN for text\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_size, hidden_size, out_size, num_layers=1, dropout=0.2, bidirectional=False):\n",
        "        '''\n",
        "        Args:\n",
        "            in_size: input dimension\n",
        "            hidden_size: hidden layer dimension\n",
        "            num_layers: specify the number of layers of LSTMs.\n",
        "            dropout: dropout probability\n",
        "            bidirectional: specify usage of bidirectional LSTM\n",
        "        Output:\n",
        "            (return value in forward) a tensor of shape (batch_size, out_size)\n",
        "        '''\n",
        "        super(TextSubNet, self).__init__()\n",
        "        self.rnn = nn.LSTM(in_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_1 = nn.Linear(hidden_size, out_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            x: tensor of shape (batch_size, sequence_len, in_size)\n",
        "        '''\n",
        "        _, final_states = self.rnn(x) #마지막으로 실행된 값: _\n",
        "        h = self.dropout(final_states[0].squeeze()) #.squeeze(): 차원에서 1 삭제\n",
        "        y_1 = self.linear_1(h)\n",
        "        return y_1"
      ],
      "metadata": {
        "id": "nmyfQJ47xJ2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TFN(nn.Module):\n",
        "    '''\n",
        "    Implements the Tensor Fusion Networks for multimodal sentiment analysis as is described in:\n",
        "    Zadeh, Amir, et al. \"Tensor fusion network for multimodal sentiment analysis.\" EMNLP 2017 Oral.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, input_dims, hidden_dims, text_out, dropouts, post_fusion_dim):\n",
        "        '''\n",
        "        Args:\n",
        "            input_dims - a length-3 tuple, contains (audio_dim, video_dim, text_dim)\n",
        "            hidden_dims - another length-3 tuple, similar to input_dims\n",
        "            text_out - int, specifying the resulting dimensions of the text subnetwork\n",
        "            dropouts - a length-4 tuple, contains (audio_dropout, video_dropout, text_dropout, post_fusion_dropout)\n",
        "            post_fusion_dim - int, specifying the size of the sub-networks after tensorfusion\n",
        "        Output:\n",
        "            (return value in forward) a scalar value between -3 and 3\n",
        "        '''\n",
        "        super(TFN, self).__init__()\n",
        "\n",
        "        # dimensions are specified in the order of audio, video and text\n",
        "        self.audio_in = input_dims[0]\n",
        "        self.video_in = input_dims[1]\n",
        "        self.text_in = input_dims[2]\n",
        "\n",
        "        self.audio_hidden = hidden_dims[0]\n",
        "        self.video_hidden = hidden_dims[1]\n",
        "        self.text_hidden = hidden_dims[2]\n",
        "        self.text_out= text_out\n",
        "        self.post_fusion_dim = post_fusion_dim\n",
        "\n",
        "        self.audio_prob = dropouts[0]\n",
        "        self.video_prob = dropouts[1]\n",
        "        self.text_prob = dropouts[2]\n",
        "        self.post_fusion_prob = dropouts[3]\n",
        "\n",
        "        # define the pre-fusion subnetworks\n",
        "        self.audio_subnet = SubNet(self.audio_in, self.audio_hidden, self.audio_prob)\n",
        "        self.video_subnet = SubNet(self.video_in, self.video_hidden, self.video_prob)\n",
        "        self.text_subnet = TextSubNet(self.text_in, self.text_hidden, self.text_out, dropout=self.text_prob)\n",
        "\n",
        "        # define the post_fusion layers\n",
        "        self.post_fusion_dropout = nn.Dropout(p=self.post_fusion_prob)\n",
        "        self.post_fusion_layer_1 = nn.Linear((self.text_out + 1) * (self.video_hidden + 1) * (self.audio_hidden + 1), self.post_fusion_dim)\n",
        "        self.post_fusion_layer_2 = nn.Linear(self.post_fusion_dim, self.post_fusion_dim)\n",
        "        self.post_fusion_layer_3 = nn.Linear(self.post_fusion_dim, 1)\n",
        "\n",
        "        # in TFN we are doing a regression with constrained output range: (-3, 3), hence we'll apply sigmoid to output\n",
        "        # shrink it to (0, 1), and scale\\shift it back to range (-3, 3)\n",
        "        self.output_range = Parameter(torch.FloatTensor([6]), requires_grad=False)\n",
        "        self.output_shift = Parameter(torch.FloatTensor([-3]), requires_grad=False)\n",
        "\n",
        "    def forward(self, audio_x, video_x, text_x):\n",
        "        '''\n",
        "        Args:\n",
        "            audio_x: tensor of shape (batch_size, audio_in)\n",
        "            video_x: tensor of shape (batch_size, video_in)\n",
        "            text_x: tensor of shape (batch_size, sequence_len, text_in)\n",
        "        '''\n",
        "        audio_h = self.audio_subnet(audio_x) # subnet 설정\n",
        "        video_h = self.video_subnet(video_x)\n",
        "        text_h = self.text_subnet(text_x)\n",
        "        batch_size = audio_h.data.shape[0]\n",
        "\n",
        "        # next we perform \"tensor fusion\", which is essentially appending 1s to the tensors and take Kronecker product\n",
        "        if audio_h.is_cuda:\n",
        "            DTYPE = torch.cuda.FloatTensor\n",
        "        else:\n",
        "            DTYPE = torch.FloatTensor\n",
        "\n",
        "        _audio_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), audio_h), dim=1)\n",
        "        _video_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), video_h), dim=1)\n",
        "        _text_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), text_h), dim=1)\n",
        "\n",
        "        # _audio_h has shape (batch_size, audio_in + 1), _video_h has shape (batch_size, _video_in + 1)\n",
        "        # we want to perform outer product between the two batch, hence we unsqueenze them to get\n",
        "        # (batch_size, audio_in + 1, 1) X (batch_size, 1, video_in + 1)\n",
        "        # fusion_tensor will have shape (batch_size, audio_in + 1, video_in + 1)\n",
        "        fusion_tensor = torch.bmm(_audio_h.unsqueeze(2), _video_h.unsqueeze(1))\n",
        "        \n",
        "        # next we do kronecker product between fusion_tensor and _text_h. This is even trickier\n",
        "        # we have to reshape the fusion tensor during the computation\n",
        "        # in the end we don't keep the 3-D tensor, instead we flatten it\n",
        "        fusion_tensor = fusion_tensor.view(-1, (self.audio_hidden + 1) * (self.video_hidden + 1), 1)\n",
        "        fusion_tensor = torch.bmm(fusion_tensor, _text_h.unsqueeze(1)).view(batch_size, -1)\n",
        "\n",
        "        post_fusion_dropped = self.post_fusion_dropout(fusion_tensor)\n",
        "        post_fusion_y_1 = F.relu(self.post_fusion_layer_1(post_fusion_dropped))\n",
        "        post_fusion_y_2 = F.relu(self.post_fusion_layer_2(post_fusion_y_1))\n",
        "        post_fusion_y_3 = F.sigmoid(self.post_fusion_layer_3(post_fusion_y_2))\n",
        "        output = post_fusion_y_3 * self.output_range + self.output_shift\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "3UrVPZQHxK2L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}